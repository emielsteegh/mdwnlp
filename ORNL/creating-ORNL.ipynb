{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORNL\n",
    "this notebook outlines how the ORNL, ORNL8 and ORNL26 datasets were created. Get them here:\n",
    "\n",
    "```python\n",
    "import datasets\n",
    "df_ornl8  = datasets.load_dataset('Rodekool/ornl8 ')\n",
    "df_ornl26 = datasets.load_dataset('Rodekool/ornl26')\n",
    "```\n",
    "<br>\n",
    "\n",
    "## Gathering the raw data\n",
    "\n",
    "1. We start by collecting the complete set of judgments, structured as a nested ZIP from [https://static.rechtspraak.nl/PI/OpenDataUitspraken.zip](https://static.rechtspraak.nl/PI/OpenDataUitspraken.zip).\n",
    "\n",
    "2. We recursively extract them into a single folder. This scripts inflates nested zips into the current dir (this takes a while...)\n",
    "\n",
    "```shell\n",
    "while [ \"`find . -type f -name '*.zip' | wc -l`\" -gt 0 ]\n",
    "do\n",
    "    find . -type f -name \"*.zip\" -exec unzip -- '{}' \\; -exec rm -- '{}' \\;\n",
    "done\n",
    "```\n",
    "\n",
    "3. We find and remove all files `<=4KB`; these only contain the boilerplate XML and have no content.\n",
    "\n",
    "```shell\n",
    "find . -name \"*.xml\" -type f -size -3k -delete\n",
    "find . -name \"*.zip\" -type f -delete\n",
    "```\n",
    "\n",
    "4. We then move the XML files into subfolders per year. Since they are all individual files this takes longer than you may wish. _note: We do this because the whole package is too much data to deal with at once for a normal computer. Since the filecount increases with the years a better option would perhaps be splitting them into chunks that fit into memory._\n",
    "\n",
    "```shell\n",
    "setopt extended_glob\n",
    "zmodload zsh/files\n",
    "setopt +o nomatch\n",
    "\n",
    "for y in {2022..1905}\n",
    "do\n",
    "        # mkdir -p $y\n",
    "        mv *_${y}_* $y\n",
    "        echo \"Moved year $y\"\n",
    "done\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-cleaning the data\n",
    "\n",
    "### XML to CSV\n",
    "All XML documents are mapped will be mapped into CSV files in the next code chunks. We keep only the relevant features (*identifier, date, case, title, short content, verdict, conclusion, category, location*). We tried different approaches to parse the files (LXML, beautiful soup, RegEx, and more), we went with beautiful soup for speed after running some tests. A loop over all documents parses them one by one using saves them as a row for the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/point/this/at/the/folder/from/the/previous/step'\n",
    "\n",
    "dir_xmls  = os.path.join(BASE, 'data','OpenDataUitspraken')\n",
    "dir_save  = os.path.join(BASE, 'data','odu_csv')\n",
    "dir_ORNL  = os.path.join(BASE, 'data','ORNL')\n",
    "dir_ORNL8  = os.path.join(BASE, 'data','ORNL8')\n",
    "dir_ORNL26  = os.path.join(BASE, 'data','ORNL20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_get_text(soup, tag):\n",
    "    text = soup.find(tag)\n",
    "    text = text.text if text else \"\"\n",
    "    return text\n",
    "\n",
    "def main_sub_split(t, part):\n",
    "    split = t.split('; ')\n",
    "    if part != 'sub':\n",
    "        return split[0]\n",
    "    else: \n",
    "        return split[1] if len(split) == 2 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all year folders with xmls into CSVs per year\n",
    "for year in sorted(os.listdir(dir_xmls)):\n",
    "    if year.isdigit():\n",
    "        if int(year) == 0:\n",
    "            print(year)\n",
    "            l = []\n",
    "            dir_year = os.path.join(dir_xmls, year)\n",
    "            for doc in sorted(os.listdir(dir_year)):\n",
    "                filename = os.path.join(dir_year, doc)\n",
    "                if doc.endswith('.xml'):\n",
    "                    with open(filename) as f:\n",
    "                        soup = bs(f, 'xml')\n",
    "                        d = {\n",
    "                            'zaak'         : soup_get_text(soup, 'psi:zaaknummer'),\n",
    "                            'identifier'   : soup_get_text(soup, 'dcterms:identifier'),\n",
    "                            'location'     : soup_get_text(soup, 'dcterms:spatial'),\n",
    "                            'category'     : soup_get_text(soup, 'dcterms:subject'),\n",
    "                            'shortcontent' : soup_get_text(soup, 'inhoudsindicatie'),\n",
    "                            'title'        : soup_get_text(soup, 'dcterms:title'),\n",
    "                            'date'         : soup_get_text(soup, 'dcterms:date'),\n",
    "                            'verdict'      : soup_get_text(soup, 'uitspraak'),\n",
    "                            'conclusion'   : soup_get_text(soup, 'conclusie'),\n",
    "                            \n",
    "                        }\n",
    "                    l.append(d)\n",
    "            \n",
    "            df = pd.DataFrame(l)\n",
    "            save_path = os.path.join(dir_save,f'{year}.csv')\n",
    "            df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests for reading and cleaning\n",
    "\n",
    "year = 2010\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_save,f'{year}.csv'), index_col=False)\n",
    "df = df.astype(str)\n",
    "df = df[df.category != 'nan']\n",
    "# to label num, title, text\n",
    "df['main_category'] = df.subject.apply(lambda x: main_sub_split(x, 'main'))\n",
    "df['sub_category']  = df.subject.apply(lambda x: main_sub_split(x, 'sub'))\n",
    "\n",
    "df['shortcontent'] = df['shortcontent'].apply(simple_clean_text)\n",
    "df['verdict'] = df['verdict'].apply(simple_clean_text)\n",
    "df['conclusion'] = df['conclusion'].apply(simple_clean_text)\n",
    "\n",
    "df_parts = []\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retaining ony instances with subcategories\n",
    "\n",
    "Next we go through all CSVs again and keep only the cases that have both a main and subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(1905,2022+1):\n",
    "    year_csv = os.path.join(dir_save,f'{year}.csv')\n",
    "    if year % 10 == 0:\n",
    "        print()\n",
    "    if os.path.exists(year_csv):\n",
    "        print('year', end=' ')\n",
    "        df = pd.read_csv(year_csv, index_col=False)\n",
    "        print(f'{year}', end=' | ')\n",
    "        df = df.astype(str)\n",
    "        df = df[df['category'] != 'nan']\n",
    "        df['sub_category']  = df.subject.apply(lambda x: main_sub_split(x, 'sub'))\n",
    "        df = df[df['sub_category'].notna()]\n",
    "        df_parts.append(df)\n",
    "\n",
    "df = pd.concat(df_parts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(dir_save,'all_subcategories_only.csv')\n",
    "df.to_csv(save_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some graphs and statistics first, nothing really happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ez = df.copy(deep=True)\n",
    "ez = ez[['sub_category', 'shortcontent', 'verdict', 'conclusion']]\n",
    "ez['shc_c'] = ez.shortcontent.str.count(' ') + 1\n",
    "ez['ver_c'] = ez.verdict.str.count(' ') + 1\n",
    "ez['con_c'] = ez.conclusion.str.count(' ') + 1\n",
    "ez = ez[['shc_c', 'ver_c', 'con_c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text statistics\n",
    "\n",
    "tab = ez.copy(deep=True)\n",
    "tab = tab.replace(1, np.nan)\n",
    "tab = tab.replace(0, np.nan)\n",
    "tab = tab.replace(2, np.nan)\n",
    "tab = tab.agg({\n",
    "    'shc_c' : ['mean', 'std', 'max', 'min', 'count'],\n",
    "    'ver_c' : ['mean', 'std', 'max', 'min', 'count'],\n",
    "    'con_c' : ['mean', 'std', 'max', 'min', 'count'],\n",
    "}).round(1)\n",
    "tab\n",
    "\n",
    "# # if you want them for a report uncomment\n",
    "# tab = tab.to_latex()\n",
    "# tab = tab.replace(\".0\",\"\")\n",
    "# print(tab)\n",
    "\n",
    "# # as percentages\n",
    "# tab.loc['in %'] = (tab.loc['count']/229172*100).round(1)\n",
    "# tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occurrences = 5_000\n",
    "max_occurrences = 30_000\n",
    "\n",
    "print(f'\\ntotal samples with a subsubcategory:  {df.shape[0]}\\n',)\n",
    "\n",
    "ss_count = df.groupby(by='sub_category', as_index=False).size()\n",
    "ss_count['percentage'] = round(ss_count['size'] / df.shape[0], 3)\n",
    "\n",
    "print('subsub-category occurences:  \\n',ss_count)\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "cropped_ss_count = ss_count[ss_count['size'] > min_occurrences]\n",
    "cropped_ss_count['percentage'] = round(cropped_ss_count['size'] / cropped_ss_count['size'].sum(), 3)\n",
    "\n",
    "print(f'cropped > {min_occurrences} subsub-category occurences:  \\n', cropped_ss_count)\n",
    "print(f'\\ntotal samples with a subsubcategory that has > {min_occurrences} occurences:  \\n', cropped_ss_count['size'].sum())\n",
    "\n",
    "print()\n",
    "\n",
    "lost_samples = ss_count['size'].sum() - cropped_ss_count['size'].sum()\n",
    "\n",
    "\n",
    "print(f'{lost_samples} ({lost_samples / ss_count[\"size\"].sum():.2f}%) samples lost by filtering ')\n",
    "\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the dataset more compact.\n",
    "\n",
    "Since the dataset is too large to reliably use, we create two seperate smaller datasets (with overlap):\n",
    "- `ORNL8` where we sample up to 30k texts from all subcategories that have at least 5k entries. Texts from subcategories with fewer than 5k entries are dropped. This leaves a total of 8 subcategories\n",
    "- `ORNL26` where we sample up to 30k texts from *all* subcategories. Leaving us with 26 distinct subcategories\n",
    "\n",
    "We also get rid of all columns except `text`, `sub_category`, and `label` (numerical encoded sub_category). Since a case will either have either a verdict or a conclusion, and sometimes a shortcontent, we turn them into one text field. `text` becomes the optional `shortcontent` + either `verdict` and `conclusion`.\n",
    "\n",
    "**IMPORTANT** In this step we drop a lot of the case's text. In our experiments we don't use it as we can only realistically run experiments up to maybe 512 tokens. So we decide to only retain 512 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlines = re.compile('(\\\\n){1,}')\n",
    "\n",
    "def simple_clean_text(t):\n",
    "    t = re.sub(newlines, ' \\n ', t)\n",
    "    return t.replace('\\t',' ').strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORNL8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcats = df.sub_category.unique()\n",
    "ls_small_df = []\n",
    "\n",
    "# filter only big categoires\n",
    "sc_count = df.groupby(by='sub_category', as_index=False).size()\n",
    "big_subcats = sc_count[ss_count['size'] > 5000].sub_category.tolist()\n",
    "big_subcats.sort()\n",
    "big_subcats\n",
    "\n",
    "max_occurences_cat = 30_000\n",
    "for subcategory in big_subcats:\n",
    "    dfx = df[df['sub_category'] == subcategory]\n",
    "    if dfx.shape[0] > max_occurences_cat:\n",
    "        dfx = dfx.sample(n=max_occurences_cat)\n",
    "    print(dfx.shape)\n",
    "    ls_small_df.append(dfx)\n",
    "    \n",
    "df_small = pd.concat(ls_small_df, ignore_index=True)\n",
    "df_small = df_small.astype(str)\n",
    "\n",
    "# fix word max and whitespace\n",
    "df_small['verdict'] = df_small['verdict'].apply(lambda x: ' '.join(x.split(' ')[:512]))\n",
    "df_small['conclusion'] = df_small['conclusion'].apply(lambda x: ' '.join(x.split(' ')[:512]))\n",
    "# do some very simple cleaning\n",
    "df_small['verdict'] = df_small['verdict'].apply(simple_clean_text)\n",
    "df_small['conclusion'] = df_small['conclusion'].apply(simple_clean_text)\n",
    "\n",
    "df_small['text'] = (df_small.shortcontent + df_small.conclusion + df_small.verdict)\n",
    "\n",
    "ornl8 = df_small[['sub_category', 'title', 'text']]\n",
    "ornl8['label'] = pd.Categorical(ornl8.sub_category)\n",
    "ornl8['label'] = ornl8.label.cat.codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcats = df.sub_category.unique()\n",
    "ls_small_df = []\n",
    "\n",
    "max_occurences_cat = 30_000\n",
    "for subcategory in subcats:\n",
    "    dfx = df[df['sub_category'] == subcategory]\n",
    "    if dfx.shape[0] > max_occurences_cat:\n",
    "        dfx = dfx.sample(n=max_occurences_cat)\n",
    "    print(dfx.shape)\n",
    "    ls_small_df.append(dfx)\n",
    "    \n",
    "df_small = pd.concat(ls_small_df, ignore_index=True)\n",
    "df_small = df_small.astype(str)\n",
    "\n",
    "# fix word max and whitespace\n",
    "df_small['verdict'] = df_small['verdict'].apply(lambda x: ' '.join(x.split(' ')[:512]))\n",
    "df_small['conclusion'] = df_small['conclusion'].apply(lambda x: ' '.join(x.split(' ')[:512]))\n",
    "# do some very simple cleaning\n",
    "df_small['verdict'] = df_small['verdict'].apply(simple_clean_text)\n",
    "df_small['conclusion'] = df_small['conclusion'].apply(simple_clean_text)\n",
    "\n",
    "df_small['text'] = (df_small.shortcontent + df_small.conclusion + df_small.verdict)\n",
    "\n",
    "ornl26 = df_small[['sub_category', 'title', 'text']]\n",
    "ornl26['label'] = pd.Categorical(ornl26.sub_category)\n",
    "ornl26['label'] = ornl26.label.cat.codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # classes8 obtained by\n",
    "# sc_count = df.groupby(by='sub_category', as_index=False).size()\n",
    "# big_subcats = sc_count[ss_count['size'] > 5000].sub_category.tolist()\n",
    "# big_subcats.sort()\n",
    "# big_subcats\n",
    "\n",
    "classes8 = [\n",
    "'Ambtenaren',\n",
    "'Arbeids',\n",
    "'Belasting',\n",
    "'Omgevings',\n",
    "'Personen- en familie',\n",
    "'Socialezekerheids',\n",
    "'Verbintenissen',\n",
    "'Vreemdelingen',\n",
    "]\n",
    "\n",
    "classes20 = [\n",
    "'Aanbestedings',\n",
    "'Ambtenaren',\n",
    "'Arbeids',\n",
    "'Belasting',\n",
    "'Bestuursproces',\n",
    "'Bestuursstraf',\n",
    "'Burgerlijk proces',\n",
    "'Europees bestuurs',\n",
    "'Europees civiel ',\n",
    "'Europees straf',\n",
    "'Goederen',\n",
    "'Insolventie',\n",
    "'Intellectueel-eigendoms',\n",
    "'Internationaal privaat',\n",
    "'Internationaal straf',\n",
    "'Materieel straf',\n",
    "'Mededingings',\n",
    "'Omgevings',\n",
    "'Ondernemings',\n",
    "'Penitentiair straf',\n",
    "'Personen- en familie',\n",
    "'Socialezekerheids',\n",
    "'Strafproces',\n",
    "'Verbintenissen',\n",
    "'Volken',\n",
    "'Vreemdelingen',\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "We've premade these split and cleaned datasets available at huggingface:\n",
    "\n",
    "```python\n",
    "import datasets\n",
    "df_ornl8  = datasets.load_dataset('Rodekool/ornl8 ')\n",
    "df_ornl26 = datasets.load_dataset('Rodekool/ornl26')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ornl_train, ornl_test = train_test_split(ornl26, train_size = 0.8)\n",
    "# ornl_train, ornl_test = train_test_split(ornl8, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(dir_ORNL26,'train.csv')\n",
    "ornl_train.to_csv(save_path, index=False, header=False)\n",
    "\n",
    "save_path = os.path.join(dir_ORNL26,'test.csv')\n",
    "ornl_test.to_csv(save_path, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (main, Dec 23 2022, 09:28:24) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
